# KNN
```python
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# 1. 데이터 로드
df = pd.read_csv('서울시_대기질_자료_제공_2022.csv')

# 2. 데이터 전처리
# '평균' 데이터 제거
df = df[df['구분'] != '평균']

# 결측치 제거
print("결측치 제거 전 데이터 수:", len(df))
df = df.dropna()
print("결측치 제거 후 데이터 수:", len(df))

# 일시를 datetime으로 변환
df['일시'] = pd.to_datetime(df['일시'])
df['hour'] = df['일시'].dt.hour
df['month'] = df['일시'].dt.month

# 미세먼지 등급 분류 함수
def categorize_dust(value):
    if value <= 30:
        return '좋음'
    elif value <= 80:
        return '보통'
    elif value <= 150:
        return '나쁨'
    else:
        return '매우나쁨'

# 3. 특성 선택 및 타겟 변환
X = df[['hour', 'month', '초미세먼지(PM2.5)']]
y = df['미세먼지(PM10)'].apply(categorize_dust)

# 4. 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. 특성 스케일링
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 6. KNN 모델 학습
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# 7. 성능 평가
y_pred = knn.predict(X_test_scaled)
print("\n분류 보고서:")
print(classification_report(y_test, y_pred))

print("\n혼동 행렬:")
print(confusion_matrix(y_test, y_pred))

# 8. 예측 예시
sample = pd.DataFrame({
    'hour': [12],
    'month': [12],
    '초미세먼지(PM2.5)': [40.0]
})
sample_scaled = scaler.transform(sample)
prediction = knn.predict(sample_scaled)
print(f"\n예측 예시:")
print(f"시간: 12시, 월: 12월, 초미세먼지: 40.0일 때")
print(f"예측된 미세먼지 등급: {prediction[0]}")
```
## 학습 목표 
특정 지역의 미세먼지 농도 예측
## 방법론
결측치는 평균으로 대체하는것이 아니라 그냥 삭제하는 방향으로 진행
이미 충분한 양의 데이터가 있어서 소량의 결측치가 중요하지 않음

## 학습 데이터 

서울시 대기질 자료 제공_2022.csv
좋음, 보통이 데이터의 대부분이라 매우나쁨의 데이터가 매우 적음
이로 인해 성능차이가 있을거 같음
## 학습 결과 

분류 보고서:
              precision    recall  f1-score   support

          나쁨       0.74      0.60      0.67      1120
        매우나쁨       0.39      0.22      0.28       163
          보통       0.83      0.81      0.82     17776
          좋음       0.87      0.89      0.88     23537
예상대로 매우 나쁨의 정확도가 낮게 나옴

## 참고 문헌

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html



# 선형 회귀 
```python
import pandas as pd  
from sklearn.linear_model import LinearRegression  
from sklearn.model_selection import train_test_split  
import numpy as np  
  
# CSV 파일 읽기  
df = pd.read_csv('서울시 대기질 자료 제공_2022.csv')  
  
# 결측치 처리  
print("결측치 처리 전:")  
print(df.isnull().sum())  
df = df.dropna()  
print("\n결측치 처리 후:")  
print(df.isnull().sum())  
  
# 월 데이터 추출  
df['월'] = pd.to_datetime(df['일시']).dt.month  
  
# 지역을 더미 변수로 변환  
df_encoded = pd.get_dummies(df, columns=['구분'])  
  
# 특성과 타겟 분리  
region_columns = []
for col in df_encoded.columns:
    if '구분_' in col:
        region_columns.append(col)

feature_columns = region_columns + ['월']

X = df_encoded[feature_columns]  
y_pm10 = df_encoded['미세먼지(PM10)']  
y_pm25 = df_encoded['초미세먼지(PM2.5)']  
  
# 데이터 분할  
X_train, X_test, y_train_pm10, y_test_pm10 = train_test_split(X, y_pm10, test_size=0.2, random_state=42)  
_, _, y_train_pm25, y_test_pm25 = train_test_split(X, y_pm25, test_size=0.2, random_state=42)  
  
# 모델 학습  
model_pm10 = LinearRegression()  
model_pm25 = LinearRegression()  
  
model_pm10.fit(X_train, y_train_pm10)  
model_pm25.fit(X_train, y_train_pm25)  
  
  
# 사용자 입력 처리 함수  
def get_prediction():  
    unique_regions = df['구분'].unique()  
    print("\n가능한 지역 목록:")  
    for i, region in enumerate(unique_regions):  
        print(f"{i}: {region}")  
  
    month = int(input("\n예측할 월을 입력하세요 (1-12): "))  
    region_idx = int(input("예측할 지역 번호를 입력하세요: "))  
  
    # 입력 데이터 변환  
    test_data = pd.DataFrame(columns=X.columns)  
    test_data.loc[0] = 0  # 모든 컬럼을 0으로 초기화  
    test_data['월'] = month  
    region_col = f'구분_{unique_regions[region_idx]}'  
    test_data[region_col] = 1  
  
    # 예측  
    pm10_pred = model_pm10.predict(test_data)[0]  
    pm25_pred = model_pm25.predict(test_data)[0]  
  
    print(f"\n예측 결과 ({unique_regions[region_idx]}, {month}월)")  
    print(f"미세먼지(PM10) 예측값: {pm10_pred:.1f} ㎍/㎥")  
    print(f"초미세먼지(PM2.5) 예측값: {pm25_pred:.1f} ㎍/㎥")  
  
  
# 모델 평가  
from sklearn.metrics import r2_score, mean_squared_error  
  
print("\n모델 성능 평가:")  
print("PM10:")  
y_pred_pm10 = model_pm10.predict(X_test)  
print(f"R2 score: {r2_score(y_test_pm10, y_pred_pm10)}")  
print(f"RMSE: {np.sqrt(mean_squared_error(y_test_pm10, y_pred_pm10))}")  
  
print("\nPM2.5:")  
y_pred_pm25 = model_pm25.predict(X_test)  
print(f"R2 score: {r2_score(y_test_pm25, y_pred_pm25)}")  
print(f"RMSE: {np.sqrt(mean_squared_error(y_test_pm25, y_pred_pm25))}")  
  
# 예측 실행  
while True:  
    try:  
        get_prediction()  
        again = input("\n다른 예측을 하시겠습니까? (y/n): ")  
        if again.lower() != 'y':  
            break  
    except Exception as e:  
        print(f"오류가 발생했습니다: {e}")  
        print("입력 값을 확인해주세요.")
```
## 학습 목표 
특정 지역의 미세먼지 농도 예측
## 방법론
선형 회귀로 선택한 이유
경험상 n월에 따라서 미세먼지 수치가 따라가는 선형적인 데이터라고 해석가능하다고 생각함
## 학습 데이터 
미세먼지 및 대기질 데이터
서울시 대기질 자료 제공_2022.csv
## 학습 결과 

모델 성능 평가:
PM10:
R2 score: 0.038
RMSE: 22.484

PM2.5:
R2 score: 0.052
RMSE: 13.659

이 모델은 성능이 매우 낮게 나옴 
이 알고리즘이 데이터에 알맞지 않은것을 선택한것으로 생각함함





# K-mean 
```python
import pandas as pd  
import numpy as np  
from sklearn.preprocessing import StandardScaler  
from sklearn.cluster import KMeans  
from sklearn.metrics import silhouette_score  
import matplotlib.pyplot as plt  
  
# 1. 데이터 읽기  
df = pd.read_csv('LOCAL_PEOPLE_GU_2023.csv')  
  
# 2. 전처리  
meta_columns = ['기준일ID', '시간대구분', '자치구코드', '총생활인구수']  
feature_columns = [col for col in df.columns if col not in meta_columns]  
  
# 3. 데이터 정규화  
scaler = StandardScaler()  
scaled_data = scaler.fit_transform(df[feature_columns])  
scaled_df = pd.DataFrame(scaled_data, columns=feature_columns)  
  
# 4. 최적의 k 찾기  
silhouette_scores = []  
k_range = range(2, 11)  
  
for k in k_range:  
    kmeans = KMeans(n_clusters=k, random_state=42)  
    clusters = kmeans.fit_predict(scaled_df)  
    score = silhouette_score(scaled_df, clusters)  
    silhouette_scores.append(score)  
    print(f'k={k}: score={score}')  
  
# 5. 최적의 k로 클러스터링  
best_k = k_range[np.argmax(silhouette_scores)]  
print(f'\n최적의 k값: {best_k}')  
  
kmeans = KMeans(n_clusters=best_k, random_state=42)  
df['Cluster'] = kmeans.fit_predict(scaled_df)  
  
# 6. 결과 출력  
print('\n각 클러스터의 크기:')  
print(df['Cluster'].value_counts())  
  
# 7. 각 클러스터의 특성 출력  
for cluster in range(best_k):  
    print(f'\n클러스터 {cluster}의 특성:')  
    cluster_data = df[df['Cluster'] == cluster]  
  
    # 평균값 계산  
    mean_values = cluster_data[feature_columns].mean()  
  
    # 상위 3개 특성 출력  
    top_features = mean_values.nlargest(3)  
    print('주요 특성:')  
    for feat, val in top_features.items():  
        print(f'{feat}: {val}')
```

## 학습 목표 
사고 발생 위치, 사고 유형, 시간대 데이터를 기반으로 **사고 패턴 군집화**
## 방법론
서울시 생활 인구 데이터를 고른 이유

시간대별, 요일별 인구 분포가 있음
지역별 특성 군집화 가능 
월단위 정기적 업데이트 

## 학습 데이터 
서울시 생활 인구 데이터 
## 학습 결과 
k=2: score=0.491
k=3:score=0.343
k=4: score=0.339
k=5: score=0.334
k=6: score=0.346
k=7: score=0.346
k=8: score=0.359
k=9: score=0.350
k=10: score=0.374

k가 높아질수록 점수가 낮아짐
즉 최적의 k는 2

클러스터 1은 77.5%
클러스터 2는 55.5%로 불균형한 분포

클러스터 1은 
- 고령 인구(여성 70세 이상)가 가장 많음
- 중년층(35-49세) 인구가 많음
- 주거 중심 지역의 특성을 보임임
클러스터 2는 
- 고령 인구와 함께 20대 여성 인구가 많음
- 젊은 층과 노년층이 혼재된 특성
- 상업/문화 중심 지역의 특성을 보임

# 참고 문헌
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html