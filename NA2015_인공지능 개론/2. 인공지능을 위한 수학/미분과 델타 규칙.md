# 미분(differentiation)과 도함수
 - 미분이란 어떤 운동이나 함수의 순간적인 움직임을 서술하는 방법
 - 어떤 함수의 미분이란 그것의 도함수를 도출해내는 과정
 - 미분 공식으로 지수함수, 로그함수, 삼각함수 등의 도함수 구함
 - 미분은 기하학적 관점에서 주어진 곡선의 접선을 구하는 문제와 같은 의미이며, 접선의 기하학적 의미는 곡선과 스치듯이 만나는 직선임

# 도함수(derivative)의 정의
 - 도함수란 정의역의 모든 x에 대해 f(x)의 미분계수로 대응시키는 함수
 - 기호로는 y′, f′(x), dy/dx로 나타내며 다음과 같이 도함수를 정의
$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$
 - f(x)의 도함수 f′(x)를 구하는 것이 미분이며, 그 계산법이 미분법
 - 함수 f가 입력값 x에 따라 변할 때 각 입력값 x에서 f의 순간변화율을 구할 수 있는데, 이때 이 함수를 f의 도함수라고 함

## 순간변화율과 도함수

 - 함수의 순간변화율은 함수의 각 점에서의 접선의 기울기를 의미
 - 도함수는 입력값 x마다 그 점에서 함수 그래프의 접선의 기울기를 대응시켜 주는 함수
 - 그림에서 x = c에서의 도함수를 나타냄
![](https://i.imgur.com/ACivpUN.png)

# 미분의 체인 규칙(chain rule)
 - 미분의 체인 규칙은 양파를 까는 것처럼 합성함수의 미분에 적용
 - 바깥 함수를 먼저 미분한 후 다시 안쪽 함수를 미분하여 곱하는 방법
 - 가령 y를 x에 관해 미분할 경우 먼저 y를 u에 관해 미분하고, 다시 u를 x에 관해 미분한 후 둘을 곱함
 - 3개 이상의 변수에 대해서도 연쇄적으로 적용 가능
$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}
$$
$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dv} \cdot \frac{dv}{dx}
$$
 - 미분의 체인 규칙은 신경망에서 학습 이론을 유도하는 바탕이 됨

# 델타 규칙(delta rule)과 경사하강법(gradient descent)
 - 델타 규칙이란 단층 신경망 모델에서 사용되는 학습 방법 중 하나
 - 실제 출력과 기대되는 출력 간의 차이를 최소화하기 위해 뉴런들 사이의 연결강도를 변화시키는데 쓰이는 방법
 - 델타 규칙을 확장한 것이 일반화 델타 규칙(generalized delta rule)
 - 1986년 Rumelhart에 의해 만들어진 다층 신경망을 학습시킬 수 있는 규칙으로, 오늘날 가장 유명해진 신경망 학습 규칙 중의 하나
 - 델타 규칙에 의해 연결강도의 변화가 연결강도 공간상에 주어지는 오차의 제곱을 높이로 하는 곡면에 대해 경사하강법을 따르는데, 즉 오차의 제곱이 가장 많이 감소하는 방향으로 변화함

# 인공지능과의 관련성
 - 인공지능 중 신경망 관련 연구는 단층 퍼셉트론의 학습 기법에 바탕
 - 신경망의 근본적인 이해가 필요한 사람은 수학적 지식 중 미분의 도함수 개념과 델타 규칙 및 체인 규칙에 대한 기초적인 이해가 필요
 - 일반화 델타 규칙 관련 도함수 전개는 저자의 “신경망 이론과 응용(I)”편에 6페이지에 걸쳐 유도되어 있을 정도로 매우 복잡함
 - 델타 규칙과 경사하강법은 신경망의 학습 규칙의 기반 이해에 필요
 - 전문가 코스가 아닌 분은 수학적 지식에 너무 스트레스 받지 않기